{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3bbb7de9",
   "metadata": {},
   "source": [
    "# üìò Hands-on Guide to Fine-Tuning Mi:dm 2.0 using TRL\n",
    "\n",
    "## üìë Table of Contents\n",
    "\n",
    "1. [Introduction](#-introduction)  \n",
    "2. [Setup Environment](#-setup-environment)  \n",
    "3. [Prepare and Load the Dataset](#-prepare-and-load-the-dataset)  \n",
    "4. [Modeling with PEFT (LoRA)](#-modeling-with-peft-lora)  \n",
    "5. [Training the Model](#-training-the-model)  \n",
    "6. [Full Fine-Tuning (Optional)](#-full-fine-tuning-optional)\n",
    "\n",
    "---\n",
    "\n",
    "## üß≠ Introduction\n",
    "\n",
    "This is the guide code that allows you to finetune our publicly available Mi:dm 2.0 with new downstream tasks.  \n",
    "Mi:dm 2.0 can be used for a variety of tasks such as question answering and summarization, etc. without any additional training.  \n",
    "However, if you want to customise the model for your application, you may need to fine-tune it to your data to get better results.  \n",
    "Please modify it according to the availability of your resources.\n",
    "\n",
    "This tutorial notebook walks you thorugh how to fine-tune Mi:dm 2.0 using Hugging Face [TRL](https://huggingface.co/docs/trl/index), [Transformers](https://huggingface.co/docs/transformers/index) & [datasets](https://huggingface.co/docs/datasets/index). In the Notebook, we are going to:\n",
    "\n",
    "1. Setup environment  \n",
    "2. Prepare and load the dataset  \n",
    "3. Fine-tune LLM using `trl` and the `SFTTrainer`\n",
    "\n",
    "---\n",
    "\n",
    "## üß∞ Setup Environment\n",
    "\n",
    "Our first step is to install Hugging Face Libraries and Pytorch, including trl, transformers and datasets.  \n",
    "TRL is a cutting-edge library designed for post-training foundation models using advanced techniques like Supervised Fine-Tuning (SFT), Proximal Policy Optimization (PPO), and Direct Preference Optimization (DPO).\n",
    "\n",
    "> ‚ö†Ô∏è **Note:** Installing Flash Attention may take a while (approximately 10‚Äì45 minutes), depending on your system performance.  \n",
    "> It's an optional optimization that can significantly speed up attention computations during training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21c8b853",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install Pytorch & other libraries\n",
    "%pip install -r requirements.txt\n",
    "import torch; assert torch.cuda.get_device_capability()[0] >= 8, 'Hardware not supported for Flash Attention'\n",
    "# install flash-attn\n",
    "!pip install ninja packaging\n",
    "!MAX_JOBS=4 pip install flash-attn==2.7.3 --no-build-isolation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8805597e",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## üìÇ Prepare and Load the Dataset\n",
    "\n",
    "In our example we‚Äôll use an already existing dataset called [simpleQA-GenX2](https://huggingface.co/datasets/KT-AI/dataset), which contains samples of natural language instructions collected in-house, including various forms of simple Korean QA.\n",
    "\n",
    "With the latest release of `trl` we now support popular instruction and conversation dataset formats.\n",
    "\n",
    "### Supported Formats\n",
    "* conversational format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64554fb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "{\"messages\": [{\"role\": \"system\", \"content\": \"You are...\"}, {\"role\": \"user\", \"content\": \"...\"}, {\"role\": \"assistant\", \"content\": \"...\"}]}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03d45c9c",
   "metadata": {},
   "source": [
    "* instruction format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a9588a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "{\"prompt\": \"<prompt text>\", \"completion\": \"<ideal generated text>\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "676d49e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "# Load dataset from the hub\n",
    "dataset = load_dataset(\"K-intelligence/KT-Simple-QA\")['train']\n",
    "dataset = dataset.train_test_split(test_size=0.01)\n",
    "\n",
    "# save datasets to disk \n",
    "dataset[\"train\"].to_json(\"train_dataset.json\", orient=\"records\")\n",
    "dataset[\"test\"].to_json(\"test_dataset.json\", orient=\"records\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "119f6d3c",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## üß† Modeling with PEFT (LoRA)\n",
    "\n",
    "### Load Mi:dm 2.0 Model and Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca9e7491",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n",
    "from trl import setup_chat_format\n",
    "\n",
    "model_id = \"K-intelligence/Midm-2.0-Mini-Instruct\"\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_id,\n",
    "    device_map=\"auto\",\n",
    "    attn_implementation=\"flash_attention_2\",\n",
    "    torch_dtype=torch.bfloat16\n",
    ")\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "930c9ad3",
   "metadata": {},
   "source": [
    "### Apply LoRA for PEFT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f800ecb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from peft import get_peft_model, LoraConfig, TaskType\n",
    "\n",
    "peft_config = LoraConfig(\n",
    "    r=8,\n",
    "    lora_alpha=16,\n",
    "    lora_dropout=0.1,\n",
    "    bias=\"none\",\n",
    "    task_type=TaskType.CAUSAL_LM,\n",
    ")\n",
    "model = get_peft_model(model, peft_config)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1e5edb2",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## üèãÔ∏è Training the Model\n",
    "\n",
    "### Load Dataset from Disk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fff4cc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "dataset = load_dataset(\"json\", data_files=\"train_dataset.json\", split=\"train\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cb6cd0c",
   "metadata": {},
   "source": [
    "### Set Training Arguments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26f28098",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import TrainingArguments\n",
    "\n",
    "args = TrainingArguments(\n",
    "    output_dir=\"save_dir\",\n",
    "    num_train_epochs=1,\n",
    "    per_device_train_batch_size=1,\n",
    "    gradient_accumulation_steps=1,\n",
    "    gradient_checkpointing=True,\n",
    "    logging_steps=10,\n",
    "    save_strategy=\"epoch\",\n",
    "    learning_rate=2e-5,\n",
    "    bf16=True,\n",
    "    max_grad_norm=1.0,\n",
    "    warmup_ratio=0.03,\n",
    "    lr_scheduler_type=\"constant\",\n",
    "    push_to_hub=False,\n",
    "    report_to=\"tensorboard\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c024f102",
   "metadata": {},
   "source": [
    "### Set Up the `SFTTrainer` and Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c48d73a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from trl import SFTTrainer\n",
    "\n",
    "trainer = SFTTrainer(\n",
    "    model=model,\n",
    "    args=args,\n",
    "    train_dataset=dataset\n",
    ")\n",
    "\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "157cd478",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## üß¨ Full Fine-Tuning (Optional)\n",
    "\n",
    "If you have enough computational resources and want to fine-tune the entire Mi:dm 2.0 model ‚Äî not just adapters ‚Äî you can follow the steps below.\n",
    "\n",
    "> ‚ö†Ô∏è **Note:** Dataset and library setup is the same as in the PEFT method above.\n",
    "\n",
    "### Load Full Mi:dm 2.0 Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bd287a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_id,\n",
    "    device_map=\"auto\",\n",
    "    attn_implementation=\"flash_attention_2\",\n",
    "    torch_dtype=torch.bfloat16\n",
    ")\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b58ba36",
   "metadata": {},
   "source": [
    "### Training Arguments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b65212e",
   "metadata": {},
   "outputs": [],
   "source": [
    "args = TrainingArguments(\n",
    "    output_dir=\"save_dir\",\n",
    "    num_train_epochs=1,\n",
    "    per_device_train_batch_size=1,\n",
    "    gradient_accumulation_steps=1,\n",
    "    gradient_checkpointing=True,\n",
    "    logging_steps=10,\n",
    "    save_strategy=\"epoch\",\n",
    "    learning_rate=2e-5,\n",
    "    bf16=True,\n",
    "    max_grad_norm=1.0,\n",
    "    warmup_ratio=0.03,\n",
    "    lr_scheduler_type=\"constant\",\n",
    "    push_to_hub=False,\n",
    "    report_to=\"tensorboard\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2acb93e3",
   "metadata": {},
   "source": [
    "### Train with SFTTrainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36c453b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from trl import SFTTrainer\n",
    "\n",
    "trainer = SFTTrainer(\n",
    "    model=model,\n",
    "    args=args,\n",
    "    train_dataset=dataset,\n",
    "    processing_class=tokenizer\n",
    ")\n",
    "\n",
    "trainer.train()"
   ]
  }
 ],
 "metadata": {
  "language": "python"
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
