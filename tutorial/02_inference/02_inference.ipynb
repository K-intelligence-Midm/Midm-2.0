{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üéØ Inference with Mi:dm 2.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook shows how to run inference with the `K-intelligence/Midm-2.0-Mini-Instruct` model using Hugging Face Transformers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ‚úÖ 1. Requirements"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We strongly recommend using `transformers >= 4.45.0`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install torch transformers accelerate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üîß 2.  Load the Model, Tokenizer, and GenerationConfig"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the model using `transformers` package.\n",
    "To obtain the best generation quality, we highly recommend using the generation config provided with the model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary>Mi:dm 2.0-Mini Optimal Generation Config</summary>\n",
    "\n",
    "```json\n",
    "{\n",
    "  \"bos_token_id\": 0,\n",
    "  \"do_sample\": true,\n",
    "  \"eos_token_id\": [2,131301],\n",
    "  \"repetition_penalty\": 1.0,\n",
    "  \"temperature\": 0.8,\n",
    "  \"top_k\": 20,\n",
    "  \"top_p\": 0.75,\n",
    "}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary>Mi:dm 2.0-Base Optimal Generation Config</summary>\n",
    "\n",
    "```json\n",
    "{\n",
    "  \"bos_token_id\": 0,\n",
    "  \"do_sample\": true,\n",
    "  \"eos_token_id\": [2, 131301],\n",
    "  \"repetition_penalty\": 1.05,\n",
    "  \"temperature\": 0.8,\n",
    "  \"top_k\": 20,\n",
    "  \"top_p\": 0.7\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, GenerationConfig\n",
    "\n",
    "model_name = \"K-intelligence/Midm-2.0-Mini-Instruct\"\n",
    "\n",
    "# Load the model\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    torch_dtype=torch.bfloat16,  \n",
    "    trust_remote_code=True,\n",
    "    device_map=\"auto\"           # Automatically uses GPU if available\n",
    ")\n",
    "\n",
    "# Load the tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "# Load generation config (controls generation behavior)\n",
    "generation_config = GenerationConfig.from_pretrained(model_name)\n",
    "\n",
    "print(\"‚úÖ Model and tokenizer loaded.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üí¨ 3. Prepare Your Prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"KTÏóê ÎåÄÌï¥ ÏÜåÍ∞úÌï¥Ï§ò\"\n",
    "\n",
    "messages = [\n",
    "    {\"role\": \"system\", \"content\": \"Mi:dm(ÎØø:Ïùå)ÏùÄ KTÏóêÏÑú Í∞úÎ∞úÌïú AI Í∏∞Î∞ò Ïñ¥ÏãúÏä§ÌÑ¥Ìä∏Ïù¥Îã§.\"},\n",
    "    {\"role\": \"user\", \"content\": prompt}\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üßæ 4. Tokenize Input with Chat Template"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_ids = tokenizer.apply_chat_template(\n",
    "    messages,\n",
    "    tokenize=True,\n",
    "    add_generation_prompt=True,\n",
    "    return_tensors=\"pt\"\n",
    ").to(model.device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üöÄ 5. Generate a Response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_ids = model.generate(\n",
    "    input_ids=input_ids,\n",
    "    generation_config=generation_config,  # Recommended config\n",
    "    eos_token_id=tokenizer.eos_token_id,\n",
    "    max_new_tokens=128\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üì§ 6. Decode and Display"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = tokenizer.decode(output_ids[0], skip_special_tokens=True)\n",
    "print(\"Model Response:\\n\", response)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
